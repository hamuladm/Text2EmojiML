{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python\\newsender\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import emoji as emj\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from langchain_community.llms import HuggingFacePipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "from transformers import pipeline\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU_MODE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:54<00:00,  6.82s/it]\n",
      "C:\\Users\\ozark\\AppData\\Local\\Temp\\ipykernel_8728\\1046219262.py:27: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
      "  llm = HuggingFacePipeline(pipeline=text_genaration_pipeline)\n"
     ]
    }
   ],
   "source": [
    "model_name = \"HuggingFaceH4/zephyr-7b-beta\"\n",
    "new_prompt = \"You are a specialized AI that only converts text into emojis. You must respond with emojis only and nothing else. Do not include any explanations, descriptions, or additional text.\"\n",
    "old_prompt = \"You are an AI specialized in converting text into emojis. Your job is to respond *only* with emojis that best represent the input text. Do not include any words, explanations, or symbols apart from emojis.\"\n",
    "if GPU_MODE is True:\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16\n",
    "    )\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config = bnb_config)\n",
    "else:\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"cpu\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "text_genaration_pipeline = pipeline(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    task = \"text-generation\",\n",
    "    temperature = 0.2,\n",
    "    do_sample=True,\n",
    "    repetition_penalty=1.1,\n",
    "    return_full_text=False,\n",
    "    max_new_tokens=20,\n",
    ")\n",
    "llm = HuggingFacePipeline(pipeline=text_genaration_pipeline)\n",
    "prompt_template = \"\"\"\n",
    "<|system|>\n",
    "You are a specialized AI that only converts text into emojis. You must respond with emojis only and nothing else. Do not include any explanations, descriptions, or additional text.\n",
    "</s>\n",
    "<|user|>\n",
    "{question}\n",
    "</s>\n",
    "<|assistant|>\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(\n",
    "     input_variables = [\"question\"],\n",
    "     template=prompt_template\n",
    " )\n",
    "llm_chain = prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_emoji(sentence):\n",
    "    pure_text = llm_chain.invoke({\"question\":sentence})\n",
    "    return ''.join(c for c in pure_text.split(\"\\n\")[0] if emj.is_emoji(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"However, a neighbour can complain to local authority about your hedge, if its height 'reduces the amenity value' of their garden, by reducing light to windows (not an issue, the gardens are too long) or denying them winter sunshine.\", 'The experiments ended in 1965.', 'The systems technology and operation is fantastic, it will have a huge impact on water conservation.', 'The two countries are locked in a dispute over U.S. allegations that China steals U.S. technology and forces U.S. companies to share trade secrets in exchange for access to the Chinese market.', 'Should You Be a CSP?', 'Our mission is to send a message to kids everywhere about the importance of staying in school and loving it!', 'Creating a memory with loved ones?', 'Children are killed and disfigured on our roads every day, and every day we fail to stop the slaughter.', 'In the countryside of Ghana, in Ecuador, and in the Philippine Islands, countless people bring the produce of their farms and their handicrafts to a town to sell.', 'The most frequent (â‰¥ 1%) adverse reactions that led to permanent discontinuation in monotherapy studies were infection, haemorrhage, multi-organ failure, and VOD.']\n"
     ]
    }
   ],
   "source": [
    "lines = 10\n",
    "with open(\"./data/relevant_data.txt\", 'r', encoding='utf-8') as file:\n",
    "    content = [file.readline().strip() for _ in range(lines)]\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python\\newsender\\venv\\Lib\\site-packages\\transformers\\models\\mistral\\modeling_mistral.py:479: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "However, a neighbour can complain to local authority about your hedge, if its height 'reduces the amenity value' of their garden, by reducing light to windows (not an issue, the gardens are too long) or denying them winter sunshine. - ğŸŒ³âŒğŸ™…ğŸ»â™‚\n",
      "The experiments ended in 1965. - ğŸ”¬ğŸ“…â³ğŸš«\n",
      "The systems technology and operation is fantastic, it will have a huge impact on water conservation. - ğŸ’§ğŸ”§ğŸ¤©ğŸŒğŸ’¡\n",
      "The two countries are locked in a dispute over U.S. allegations that China steals U.S. technology and forces U.S. companies to share trade secrets in exchange for access to the Chinese market. - ğŸ¤ğŸ”ğŸ›‘\n",
      "Should You Be a CSP? - ğŸ’»ğŸ“ŠğŸ”‘â—\n",
      "Our mission is to send a message to kids everywhere about the importance of staying in school and loving it! - ğŸ“ğŸ’­ğŸ§ ğŸ“ğŸ˜\n",
      "Creating a memory with loved ones? - ğŸ“¸ğŸ‘¨ğŸ‘©\n",
      "Children are killed and disfigured on our roads every day, and every day we fail to stop the slaughter. - ğŸš—ğŸš¶â™‚ğŸš¶â™€\n",
      "In the countryside of Ghana, in Ecuador, and in the Philippine Islands, countless people bring the produce of their farms and their handicrafts to a town to sell. - ğŸ“ğŸ…ğŸŠğŸ‡ğŸ\n",
      "The most frequent (â‰¥ 1%) adverse reactions that led to permanent discontinuation in monotherapy studies were infection, haemorrhage, multi-organ failure, and VOD. - ğŸ¤§ğŸ”ğŸ˜µ\n"
     ]
    }
   ],
   "source": [
    "for sentence in content:\n",
    "    emoji = text_to_emoji(sentence)\n",
    "    print(f\"{sentence} - {emoji}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = llm_chain.invoke({\"context\":\"Please give your answers strictly according to the following template and do not provide any additional information:\\n<sentence in text> - <emoji translation>\", \"question\":\"generate random sentence and it's emoji variant\"})\n",
    "# pure_text = result.split(\"<|assistant|>\")[1].strip()\n",
    "# print(pure_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for text in pure_text:\n",
    "#     sent, emoji = text.split(\"-\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
